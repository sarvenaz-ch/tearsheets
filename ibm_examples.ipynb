{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9992b29a-9150-4a20-9436-c6530f2cdcbe",
   "metadata": {},
   "source": [
    "## IBM examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97dee060-0550-44c8-8905-db5b6795db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "from dotenv import load_dotenv\n",
    "from genai.credentials import Credentials\n",
    "from genai.model import Model\n",
    "from genai.schemas import GenerateParams, ModelType\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "api_url = os.getenv(\"GENAI_API\", None)\n",
    "creds = Credentials(api_key, api_endpoint=api_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5c47b-46d7-4ae5-a50e-46d1848678f6",
   "metadata": {},
   "source": [
    "### Example 1: Generated text between two chatbots\n",
    " - Updated to put limit on text exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836391e5-16f8-4e45-a8f5-2642111ee4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Example (Model Talk)-------------\n",
      "\n",
      "[Alice] --> Hello! How are you?\n",
      "[Bob] --> Good. You?\n",
      "[Alice] --> good too\n",
      "[Bob] --> too good\n",
      "[Alice] --> not too clean\n",
      "[Bob] --> boasting a three star rating, the hotel offers guests a quiet location which is only a short walk to the\n",
      "[Alice] --> city centre, pier and harbour. to maintain that sense of personal attention, the owner lived in one of the rooms and regularly visited guests\n",
      "[Bob] --> the personal touch - the owner had actually lived in this hotel throughout, and arranges it's running between his holidays\n",
      "[Alice] --> From the large outside pool (tiny swimming pool) that runs alongside hotel via shady pathways (used for dogs unfortunately) to the much quieter inner courtyard, the penthouse was much more than we\n",
      "[Bob] --> Excellent location in terms of entertaining and restaurants.Very good value for money with air conditioned room and close to Metro.\n",
      "[Alice] --> Not enjoyed the noise from the dance floor adjacent to our room, hence had to keep the door open an extra hole for noise to empty.\n",
      "[Bob] --> The buffet passage was dirty.\n",
      "[Alice] --> It took a long time to get to the buffet, even when we arrived early. I was disappointed with the quality of the food served.\n",
      "[Bob] --> It took several minutes for the waitress to take our order, then to bring it. Unfortunately, then food took over 40\n",
      "[Alice] --> minutes to come out of the door. Unfortunately first and second waitress gave me attitude when telling me to give them our credit card, saying she was planning to turn the usual suspects away, but I should have found the\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------- Example (Model Talk)-------------\\n\")\n",
    "\n",
    "bob_params = GenerateParams(decoding_method=\"sample\", max_new_tokens=25, temperature=1)\n",
    "alice_params = GenerateParams(decoding_method=\"sample\", max_new_tokens=45, temperature=0)\n",
    "bob = Model(ModelType.FLAN_UL2, params=bob_params, credentials=creds)\n",
    "alice = Model(ModelType.FLAN_T5, params=alice_params, credentials=creds)\n",
    "\n",
    "count = 0\n",
    "sentence = \"Hello! How are you?\"\n",
    "\n",
    "print(f\"[Alice] --> {sentence}\")\n",
    "while count < 7:\n",
    "    bob_response = bob.generate([sentence])\n",
    "    # from first batch get first result generated text\n",
    "    bob_gen = bob_response[0].generated_text\n",
    "    print(f\"[Bob] --> {bob_gen}\")\n",
    "\n",
    "    alice_response = alice.generate([bob_gen])\n",
    "    # from first batch get first result generated text\n",
    "    alice_gen = alice_response[0].generated_text\n",
    "    print(f\"[Alice] --> {alice_gen}\")\n",
    "\n",
    "    sentence = alice_gen\n",
    "    count += 1\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126cb096-e253-4aca-8c47-0bc12912cdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GenerateResult(generated_text='It took several minutes for the waitress to take our order, then to bring it. Unfortunately, then food took over 40', generated_token_count=25, input_token_count=30, stop_reason='MAX_TOKENS', generated_tokens=None, input_text='It took a long time to get to the buffet, even when we arrived early. I was disappointed with the quality of the food served.', seed=4285672223)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb85cb50-3cf7-42ff-b2eb-29b0a852be94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GenerateResult(generated_text='a=2 b=2 c=2 d=2 aw=2 bbw=2 camw=2 count=2 while(aw=b and ', generated_token_count=45, input_token_count=21, stop_reason='MAX_TOKENS', generated_tokens=None, input_text='please count to 10 slowly in steps of 2. output result as a python list', seed=2916260101)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice.generate(['please count to 10 slowly in steps of 2. output result as a python list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06598d10-b2d1-4b16-983e-9b018738b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: generated_text='A motorcycle driver swerved into another driver lane who then sped up from fear.' generated_token_count=23 input_token_count=6 stop_reason='EOS_TOKEN' generated_tokens=None input_text='this is a sentence' seed=881481999\n",
      "Generated text: generated_text='As the anchovy swam in the water the whales approached to meet him' generated_token_count=21 input_token_count=5 stop_reason='EOS_TOKEN' generated_tokens=None input_text='this is another sentence' seed=2469003477\n"
     ]
    }
   ],
   "source": [
    "responses = bob.generate_as_completed(['this is a sentence', 'this is another sentence'])\n",
    "for response in responses:\n",
    "    print(f\"Generated text: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef6fcf72-38d3-46a6-91cf-73c4aed5deed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: generated_text='What say you, Zed?' generated_token_count=8 input_token_count=11 stop_reason='EOS_TOKEN' generated_tokens=None input_text='this is a sentence \\\\ what say you' seed=3708855024\n"
     ]
    }
   ],
   "source": [
    "responses = bob.generate_as_completed([' \\ '.join(['this is a sentence', 'what say you'])])\n",
    "for response in responses:\n",
    "    print(f\"Generated text: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de2e228d-1dcf-40eb-bee1-dae9c8ddc73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mbob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_as_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPromptPattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateResponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "The generate endpoint is the centerpiece of the GENAI alpha.\n",
       "It provides a simplified and flexible, yet powerful interface to the supported\n",
       "models as a service. Given a text prompt as inputs, and required parameters\n",
       "the selected model (model_id) will generate a completion text as generated_text.\n",
       "\n",
       "Args:\n",
       "    prompts (list[str]): The list of one or more prompt strings.\n",
       "\n",
       "Yields:\n",
       "    Generator[GenerateResult]: A generator of results\n",
       "\u001b[0;31mFile:\u001b[0m      ~/venv/llm/lib/python3.11/site-packages/genai/model.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bob.generate_as_completed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c14a71-9e5f-4ba4-8375-89702616b458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
